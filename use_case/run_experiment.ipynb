{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlogging\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m \n\u001b[1;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmlca_for_elec\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnetworks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmain\u001b[39;00m \u001b[39mimport\u001b[39;00m eval_config\n\u001b[0;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\MLICA-for-elec\\src\\mlca_for_elec\\networks\\main.py:13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mseaborn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msns\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptim\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39moptim\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pieples\\AppData\\Local\\anaconda3\\lib\\site-packages\\seaborn\\__init__.py:5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \u001b[39m# noqa: F401,F403\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpalettes\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \u001b[39m# noqa: F401,F403\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mrelational\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \u001b[39m# noqa: F401,F403\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mregression\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \u001b[39m# noqa: F401,F403\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcategorical\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \u001b[39m# noqa: F401,F403\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pieples\\AppData\\Local\\anaconda3\\lib\\site-packages\\seaborn\\relational.py:17\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_oldcore\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     VectorPlotter,\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     12\u001b[0m     locator_to_legend_entries,\n\u001b[0;32m     13\u001b[0m     adjust_legend_subtitles,\n\u001b[0;32m     14\u001b[0m     _default_color,\n\u001b[0;32m     15\u001b[0m     _deprecate_ci,\n\u001b[0;32m     16\u001b[0m )\n\u001b[1;32m---> 17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_statistics\u001b[39;00m \u001b[39mimport\u001b[39;00m EstimateAggregator\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39maxisgrid\u001b[39;00m \u001b[39mimport\u001b[39;00m FacetGrid, _facet_docs\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_docstrings\u001b[39;00m \u001b[39mimport\u001b[39;00m DocstringComponents, _core_docs\n",
      "File \u001b[1;32mc:\\Users\\pieples\\AppData\\Local\\anaconda3\\lib\\site-packages\\seaborn\\_statistics.py:31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 31\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstats\u001b[39;00m \u001b[39mimport\u001b[39;00m gaussian_kde\n\u001b[0;32m     32\u001b[0m     _no_scipy \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\pieples\\AppData\\Local\\anaconda3\\lib\\site-packages\\scipy\\stats\\__init__.py:485\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m.. _statsrefmanual:\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    480\u001b[0m \n\u001b[0;32m    481\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    483\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_warnings_errors\u001b[39;00m \u001b[39mimport\u001b[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001b[0;32m    484\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[1;32m--> 485\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_stats_py\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    486\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_variation\u001b[39;00m \u001b[39mimport\u001b[39;00m variation\n\u001b[0;32m    487\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdistributions\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\pieples\\AppData\\Local\\anaconda3\\lib\\site-packages\\scipy\\stats\\_stats_py.py:46\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspecial\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mspecial\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m \u001b[39mimport\u001b[39;00m linalg\n\u001b[1;32m---> 46\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m distributions\n\u001b[0;32m     47\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _mstats_basic \u001b[39mas\u001b[39;00m mstats_basic\n\u001b[0;32m     48\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_stats_mstats_common\u001b[39;00m \u001b[39mimport\u001b[39;00m (_find_repeats, linregress, theilslopes,\n\u001b[0;32m     49\u001b[0m                                    siegelslopes)\n",
      "File \u001b[1;32mc:\\Users\\pieples\\AppData\\Local\\anaconda3\\lib\\site-packages\\scipy\\stats\\distributions.py:16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_levy_stable\u001b[39;00m \u001b[39mimport\u001b[39;00m levy_stable\n\u001b[0;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_discrete_distns\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_entropy\u001b[39;00m \u001b[39mimport\u001b[39;00m entropy\n\u001b[0;32m     18\u001b[0m \u001b[39m# For backwards compatibility e.g. pymc expects distributions.__all__.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mrv_discrete\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrv_continuous\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrv_histogram\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mentropy\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1002\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:945\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1439\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1411\u001b[0m, in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1544\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:147\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import logging\n",
    "import random \n",
    "from mlca_for_elec.networks.main import eval_config\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from mlca_for_elec.env.env import Microgrid, HouseHold\n",
    "from mlca_for_elec.mlca_elec.mlca import *\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "os.chdir(\"c:\\\\Users\\\\pieples\\\\Documents\\\\MLICA-for-elec\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_number = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "household_path = f\"config\\experiment{exp_number}\\households\"\n",
    "microgrid_path = f\"config\\experiment{exp_number}\\microgrid\\exp{exp_number}_microgrid.json\"\n",
    "dataset_path = f\"config\\experiment{exp_number}\\dataset\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "print(\"Start loading household profiles\")\n",
    "folder_path = household_path\n",
    "houses = []\n",
    "\n",
    "microgrid_1 =json.load(open( microgrid_path))\n",
    "\n",
    "\n",
    "for file in os.listdir(folder_path)[:5]:\n",
    "    if file.endswith(\".json\"):\n",
    "        household = json.load(open(folder_path+\"/\"+ file))\n",
    "    house = HouseHold(household, microgrid_1[\"horizon\"])\n",
    "\n",
    "    generation_path = \"data\\solar_prod\\Timeseries_55.672_12.592_SA2_1kWp_CdTe_14_44deg_-7deg_2020_2020.csv\"\n",
    "    consumption_path = f\"data/consumption/Reference-{house.param['consumption']['type']}.csv\"\n",
    "    spot_price_path = \"data/spot_price/2020.csv\"\n",
    "    fcr_price_path = \"data/fcr_price/random_fcr.csv\"\n",
    "    profile_path_train = dataset_path + f\"/dataset_{house.ID}.csv\"\n",
    "    profile_path_valtest = dataset_path + f\"/test_dataset_{house.ID}.csv\"\n",
    "    house.load_data(generation_path,consumption_path, spot_price_path,fcr_price_path, profile_path_train, profile_path_valtest,type = float)\n",
    "    for i in range(1):\n",
    "        house.next_data()\n",
    "    houses.append(house)\n",
    "print(f\"Loaded {len(houses)} households\")\n",
    "print(\"Start compute social welfare\")\n",
    "print(list(houses[0].data['consumption'].to_numpy()))\n",
    "\n",
    "MG = Microgrid(houses, microgrid_1)\n",
    "optimal_allocation = {}\n",
    "for house in MG.households:\n",
    "    print(house.data['consumption'].sum())\n",
    "    optimal_allocation_tuple = MG.get_efficient_allocation()\n",
    "    optimal_allocation[house.ID] = (optimal_allocation_tuple[0][house.ID] , MG.calculate_value(house.ID, optimal_allocation_tuple[0][house.ID]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MG life signal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of households: {len(MG.households)}\")    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display experiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data per houses \n",
    "fig, ax = plt.subplots(len(MG.households),1, figsize = (30,10))\n",
    "for i,house in enumerate(MG.households):\n",
    "    ax[i].plot(house.df['consumption'], label = \"consumption\")\n",
    "    ax[i].set_title(f\"Household {house.ID}\")\n",
    "    for t in range(0,len(house.df),24):\n",
    "        ax[i].axvline(t, color = \"red\", alpha = 0.5)\n",
    "plt.suptitle(\"Consumption\")\n",
    "plt.show()\n",
    "\n",
    "# plot data per houses \n",
    "fig, ax = plt.subplots(len(MG.households),1, figsize = (30,10))\n",
    "for i,house in enumerate(MG.households):\n",
    "    ax[i].plot(house.df['generation'], label = \"Generation\")\n",
    "    ax[i].set_title(f\"Household {house.ID}\")\n",
    "plt.suptitle(\"Generation\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# MG.generate_dataset(1,dataset_path,1000)\n",
    "# MG.generate_test_dataset(1,dataset_path,200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df = pd.read_csv(f\"config\\experiment{exp_number}\\dataset/dataset_1.csv\", index_col = 0)\n",
    "df.value.hist(bins = 100)\n",
    "plt.axvline(optimal_allocation[1][1], color = \"red\", alpha = 0.5, label = \"value of optimal allocation\")\n",
    "plt.xlabel(\"Valuation\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.title (f\"Distribution of valuation of households consumption after initial sampling_{len(df)} samples, dim = {len(df.columns)-1}\")\n",
    "plt.show()\n",
    "df.drop(columns=\"value\").sum(axis = 1).hist(bins = 100)\n",
    "df.drop(columns=\"value\").sum(axis = 1)\n",
    "plt.axvline(optimal_allocation[1][0].sum(), color = \"red\", alpha = 0.5, label= \" Total energy of optimal allocation\")\n",
    "plt.xlabel(\"Total energy\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(f\"Distribution of total energy of households consumption after initial sampling_{len(df)} samples, dim = {len(df.columns)-1}\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.scatter(df.drop(columns=\"value\").sum(axis = 1), df.value, alpha = 1)\n",
    "plt.scatter(optimal_allocation[1][0].sum(), optimal_allocation[1][1], color = \"red\", alpha = 1, label = \"optimal allocation\")\n",
    "plt.xlabel(\"Total energy\")\n",
    "plt.ylabel(\"Valuation\")\n",
    "plt.title(f\"Total energy vs valuation of households consumption after initial sampling_{len(df)} samples, dim = {len(df.columns)-1}\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# df.drop(columns=\"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for traj in df.drop(columns=\"value\")[:1000].to_numpy():\n",
    "    plt.plot(traj, color = \"r\", alpha=0.1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Hyper Parameters optimization\n",
    "\n",
    "\n",
    "for hyperparamter optimizations we use optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_network(cfg: dict, seed: int, MicroGrid_instance: str, bidder_id: str, num_train_data: int, layer_type: str,\n",
    "                     normalize: bool, normalize_factor: float, eval_test=False, save_datasets=False):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    return eval_config(\n",
    "        seed=seed, SAT_instance=MicroGrid_instance, bidder_id=bidder_id,\n",
    "        layer_type=layer_type, batch_size=cfg['batch_size'], num_hidden_layers=cfg['num_hidden_layers'],\n",
    "        num_hidden_units=int(max(1, np.round(cfg['num_neurons'] / cfg['num_hidden_layers']))), l2=cfg['l2'], l1 =cfg['l1'],\n",
    "        lr=cfg['lr'], normalize_factor=normalize_factor, optimizer=cfg['optimizer'], num_train_data=num_train_data,\n",
    "        eval_test=True, epochs=cfg['epochs'], loss_func=cfg['loss_func'], normalize=normalize, save_datasets=False, log_path=\"logs\", ts = cfg[\"ts\"], state_dict = cfg[\"state_dict\"], plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load bestparam \n",
    "%matplotlib inline\n",
    "import optuna\n",
    "bidder_id = 1\n",
    "nitems=50\n",
    "\n",
    "layer = \"CALayerReLUProjected\"\n",
    "# layer = \"PlainNN\"\n",
    "print(f\"Exp_{exp_number}_bidder{bidder_id}_nitems_{nitems}_layer_{layer}\")\n",
    "study = optuna.load_study(study_name=f\"Exp_{exp_number}_bidder{bidder_id}_nitems_{nitems}_layer_{layer}\", storage=\"sqlite:///db_sqlite.db\")\n",
    "config_dict = study.best_params\n",
    "print(config_dict)\n",
    "# add relevant parameters\n",
    "\n",
    "config_dict[\"ts\"] = 1\n",
    "config_dict[\"loss_func\"] = \"F.l1_loss\"\n",
    "config_dict[\"state_dict\"] = None\n",
    "# base parameters\n",
    "\n",
    "normalize_factor = 1\n",
    "\n",
    "# Run model\n",
    "\n",
    "print('Selected hyperparameters', config_dict)\n",
    "model, logs = evaluate_network(\n",
    "        config_dict, seed=0, MicroGrid_instance=MG, bidder_id=bidder_id,\n",
    "        num_train_data = nitems  ,layer_type=config_dict[\"model\"],\n",
    "        normalize=True,\n",
    "        normalize_factor= normalize_factor)\n",
    "train_logs = logs['metrics']['train'][config_dict['epochs']]\n",
    "val_logs = logs['metrics']['val'][config_dict['epochs']]\n",
    "test_logs = logs['metrics']['test'][config_dict['epochs']]\n",
    "print('Train metrics \\t| mae: {:.3f}, pearson corr.: {:.3f}, KT: {:.3f}'.format(train_logs['mae'], train_logs['r'], train_logs['kendall_tau']))\n",
    "print('Valid metrics \\t| mae: {:.3f}, pearson corr.: {:.3f}, KT: {:.3f}'.format(val_logs['mae'], val_logs['r'], val_logs['kendall_tau']))\n",
    "print('Test metrics \\t| mae: {:.3f}, pearson corr.: {:.3f}, KT: {:.3f}'.format(test_logs['mae'], test_logs['r'], test_logs['kendall_tau']))\n",
    "best_result = (test_logs['mae'], config_dict, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "best_model = best_result[2]\n",
    "profile_train = pd.read_csv(f\"config\\experiment{exp_number}\\dataset/dataset_1.csv\")\n",
    "bids = profile_train.to_numpy()[:,1:-1]\n",
    "value = profile_train.to_numpy()[:,-1]\n",
    "n_item = nitems\n",
    "\n",
    "bids_train = torch.Tensor(bids[:n_item])\n",
    "target_train = value[:n_item]\n",
    "target_max = target_train.max()* (1 / normalize_factor)\n",
    "n_item = 1000000\n",
    "profile_test = pd.read_csv(f\"config\\experiment{exp_number}\\dataset/test_dataset_1.csv\", nrows = n_item)\n",
    "bids = profile_test.to_numpy()[:,1:-1]\n",
    "value = profile_test.to_numpy()[:,-1]\n",
    "\n",
    "bids_val = torch.Tensor(bids)[:int(len(bids)*0.2)]\n",
    "target_val = value[:int(len(bids)*0.2)]\n",
    "\n",
    "bids_test = torch.Tensor(bids)[int(len(bids)*0.2):]\n",
    "null_test = torch.zeros_like(bids_test[1])\n",
    "target_test = value[int(len(bids)*0.2):]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Scale targets\n",
    "target_train = target_train /target_max\n",
    "target_val = target_val /target_max\n",
    "target_test = target_test /target_max\n",
    "\n",
    "\n",
    "pred_train = best_model(bids_train)\n",
    "pred_val = best_model(bids_val)\n",
    "pred_test = best_model(bids_test)\n",
    "pred_null = best_model(null_test)\n",
    "\n",
    "#plt.xlim(-0.1, target_max*1.1)#\n",
    "# plt.ylim(-0.1, target_max*1.1)#\n",
    "dat_min, dat_max = min(min(pred_train), min(target_train)), \\\n",
    "                           max(max(pred_train), max(target_train))\n",
    "fig, axs = plt.subplots(2,1,figsize=(5,6), gridspec_kw={'height_ratios': [3, 1]})\n",
    "\n",
    "axs[0].plot([0,target_train.max()*target_max], [0, normalize_factor*target_max], color = \"black\", alpha=0.5)\n",
    "#plt.plot([dat_min, dat_max], [dat_min, dat_max], 'y')\n",
    "axs[0].scatter(target_test*target_max, pred_test.detach().numpy()*target_max, label = \"test\", marker=\"o\", color = \"r\", alpha=0.1,s=5)\n",
    "axs[0].scatter(target_val*target_max, pred_val.detach().numpy()*target_max, label = \"validation\", marker=\"o\", color = \"g\", alpha=0.1,s=5)\n",
    "# plt.scatter(target_synth*target_max, pred_synth.detach().numpy()*target_max, marker=\"o\", color = \"r\", alpha =0.1, s=5)\n",
    "axs[0].scatter(target_train*target_max, pred_train.detach().numpy()*target_max, label = \"train\", marker=\"x\", color = \"b\", s=5)\n",
    "axs[0].scatter(0, pred_null.detach().numpy()*target_max, label = \"null\", marker=\"o\", color = \"g\", s=100)\n",
    "# plt.scatter(optimal_allocation[1][1],  best_model(torch.Tensor(optimal_allocation[1][0])).detach().numpy()*target_max, label = \"optimal\", marker=\"o\", color = \"r\", s=100)\n",
    "axs[0].set_xlabel(\"True value\")\n",
    "axs[0].set_ylabel(\"Predicted value\")\n",
    "plt.suptitle(f\"Model : {layer}, nitems : {nitems}, bidder : {bidder_id}\")\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].hist(target_test*target_max, bins=100, label = \"target\")\n",
    "axs[1].hist( pred_test.detach().numpy()*target_max, bins = 100, alpha = 0.5, label = \"prediction\")\n",
    "axs[1].set_xlabel(\"Value\")\n",
    "axs[1].set_ylabel(\"Frequency\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_test.shape,pred_test.detach().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = best_result[2]\n",
    "conso = MG.households[0].data[\"consumption\"]\n",
    "def calculate_value(bid):\n",
    "    return sum(map(lambda x: min(x[0],x[1])*0.2, zip(bid, conso)))\n",
    "\n",
    "X,Y = np.meshgrid(np.linspace(0,4,100),np.linspace(0,4,100))\n",
    "C= 0*np.zeros((100,100))\n",
    "C_opt= 0*np.zeros((100,100))\n",
    "vec =1*np.ones(MG.horizon)\n",
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "            vec[1] = X[i,j]\n",
    "            vec[2] = Y[i,j]\n",
    "            C[i,j] = best_model(torch.tensor(vec).float()).detach().numpy()*target_max\n",
    "            C_opt[i,j] = calculate_value(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "print(conso)\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "#ax.contour(X, Y, C, levels =20)\n",
    "c = ax.contourf(X, Y, C, levels =50, cmap=\"viridis\")\n",
    "ax.set_xlabel('Qtty in Hour 0 kW')\n",
    "ax.set_ylabel('Qtty in Hour 1 kW')\n",
    "ax.axhline(conso[2], color = \"black\", label = \"conso hour 1\", linestyle=\"--\")\n",
    "ax.axvline(conso[1], color = \"red\", label = \"conso hour 0\", linestyle=\"--\")\n",
    "\n",
    "ax.legend()\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "fig.colorbar(c, cax=cax, orientation='vertical')\n",
    "ax.set_title(f\"Display of prediction, model : {layer}, nitems : {nitems}, bidder : {bidder_id}\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLCA auctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import optuna\n",
    "Qinit =nitems\n",
    "Qmax = nitems + 30\n",
    "Qround=1\n",
    "L=3000\n",
    "sample_weight_on = False\n",
    "sample_weight_scaling = None\n",
    "min_iteration = 1\n",
    "seed_instance = 12\n",
    "model_name = 'MVNN'\n",
    "Mip_bounds_tightening = \"IA\"\n",
    "warm_start=False\n",
    "NN_parameters = {f\"Bidder_{i}\" : {} for i in range(len(MG.households))}\n",
    "loacal_scaling_factor = 1\n",
    "\n",
    "\n",
    "\n",
    "NN_parameters = defaultdict(dict)\n",
    "\n",
    "\n",
    "\n",
    "study = optuna.load_study(study_name=f\"Exp_{exp_number}_bidder{bidder_id}_nitems_{nitems}_layer_{layer}\", storage=\"sqlite:///db_sqlite.db\")\n",
    "config_dict = study.best_params\n",
    "print(config_dict)\n",
    "# add relevant parameters\n",
    "\n",
    "config_dict[\"ts\"] = 1\n",
    "config_dict[\"loss_func\"] = \"F.l1_loss\"\n",
    "config_dict[\"state_dict\"] = {0 : None, 1 : None, 2 :None, 3 : None, 4: None, 5: None} \n",
    "# base parameters\n",
    "\n",
    "normalize_factor = 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# {0 : None, 1 : None, 2 :None} \n",
    "#  {0 : \"config\\experiment1\\model_0.pt\", 1 : \"config\\experiment1\\model_1.pt\", 2 : \"config\\experiment1\\model_2.pt\"}\n",
    "for house in MG.households:\n",
    "    for key, value in config_dict.items():\n",
    "        if key == \"state_dict\":\n",
    "            NN_parameters[f\"Bidder_{house.ID}\"]['state_dict'] = value[house.ID]\n",
    "        else:    \n",
    "            NN_parameters[f\"Bidder_{house.ID}\"][key] = value\n",
    "    \n",
    "    NN_parameters[f\"Bidder_{house.ID}\"]['layer_type'] = config_dict[\"model\"]    \n",
    "\n",
    "    NN_parameters[f\"Bidder_{house.ID}\"]['num_hidden_units'] = int(max(1, np.round(\n",
    "        NN_parameters[f\"Bidder_{house.ID}\"]['num_neurons'] / NN_parameters[f\"Bidder_{house.ID}\"]['num_hidden_layers'])))\n",
    "    NN_parameters[f\"Bidder_{house.ID}\"].pop('num_neurons')\n",
    "\n",
    "# NN_parameters = value_model.parameters_to_bidder_id(NN_parameters)\n",
    "\n",
    "\n",
    "MIP_parameters = {\n",
    "        'bigM': 3000,\n",
    "        'mip_bounds_tightening': None,\n",
    "        'warm_start': False,\n",
    "        'time_limit' :300,\n",
    "        'relative_gap': 1e-2,\n",
    "        'integrality_tol': 1e-6,\n",
    "        'attempts_DNN_WDP': 5\n",
    "    }\n",
    "\n",
    "\n",
    "res_path = f\"results/Exp_{exp_number}_bidder{bidder_id}_Qinit_{nitems}_layer_{layer}.json\"\n",
    "\n",
    "\n",
    "RESULT = mlca_mechanism(value_model = MG, \n",
    "    SATS_auction_instance_seed=None,\n",
    "    \n",
    "    Qinit = Qinit,\n",
    "    Qmax = Qmax,\n",
    "    Qround = Qround,\n",
    "    MIP_parameters=MIP_parameters,\n",
    "    NN_parameters=NN_parameters,\n",
    "    res_path =res_path,\n",
    "    scaler=None,\n",
    "    calc_efficiency_per_iteration=True, \n",
    "    local_scaling_factor=loacal_scaling_factor,\n",
    "    return_payments = True,\n",
    "    )\n",
    "\n",
    "print(RESULT[1]['MLCA Payments'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
